# -*- coding: UTF-8 -*-.
import argparse
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.grid_search import GridSearchCV
import tflearn
import matplotlib.pyplot as plt
import numpy as np
import sys
sys.path.append("./lib")
from common import read_file, cross_validation, hot_enconding, concatenate_features, prob_to_class, prob_to_class_threshold
import gensim
from collections import defaultdict

N_GOODWARES = 100
N_MALWARES = 1000
N_FEATURES = 100
N_EXECUTIONS = 10
GW_TEST_SIZE = 0.1
MW_TEST_SIZE = 0.1
THRESHOLD = False

def params():
    parser = argparse.ArgumentParser()
    parser.add_argument('classifier', help='Classifier')
    parser.add_argument('goodwares_csv', help='Goodwares CSV location')
    parser.add_argument('malwares_csv', help='Malwares CSV Location')
    params = parser.parse_args()
    return params.classifier, params.goodwares_csv, params.malwares_csv

class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(word2vec.itervalues().next())

    def fit(self, X):
        return self

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])

class TfidfEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.word2weight = None
        self.dim = len(word2vec.itervalues().next())

    def fit(self, X):
        tfidf = TfidfVectorizer(analyzer=lambda x: x)
        tfidf.fit(X)
        # if a word was never seen - it must be at least as infrequent
        # as any of the known words - so the default idf is the max of
        # known idf's
        max_idf = max(tfidf.idf_)
        self.word2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

        return self

    def transform(self, X):
        return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
            ])


# get params
classifier, gw_csv, mw_csv = params()

# read goodwares and malwares
gw_features, gw_identifiers, gw_dlls, gw_symbols = read_file(gw_csv)
mw_features, mw_identifiers, mw_dlls, mw_symbols, years = read_ymd_csvs(mw_csv)

print "Goodwares:",len(gw_features)
print "Malwares:", len(mw_features)

# create label arrays
gw_label = np.zeros(len(gw_features))
mw_label = np.ones(len(mw_features))

accs = []
f1s = []
precs = []
recs = []

for exe in range(N_EXECUTIONS):

    # cross validation

    # goodwares cross validation
    gw_features_train, gw_features_test, gw_identifiers_train, gw_identifiers_test, gw_dlls_train, gw_dlls_test, gw_symbols_train, gw_symbols_test, gw_labels_train, gw_labels_test = cross_validation(gw_features, gw_identifiers, gw_dlls, gw_symbols, gw_label,test_size=GW_TEST_SIZE)

    # malwares cross validation
    mw_features_train, mw_features_test, mw_identifiers_train, mw_identifiers_test, mw_dlls_train, mw_dlls_test, mw_symbols_train, mw_symbols_test, mw_labels_train, mw_labels_test = cross_validation(mw_features, mw_identifiers, mw_dlls, mw_symbols, mw_label,test_size=MW_TEST_SIZE)

    # create train and test sets - concatenate goodwares and malwares
    features_train = np.concatenate((gw_features_train, mw_features_train),axis=0)
    features_test = np.concatenate((gw_features_test, mw_features_test),axis=0)
    identifiers_train = np.concatenate((gw_identifiers_train, mw_identifiers_train),axis=0)
    identifiers_test = np.concatenate((gw_identifiers_test, mw_identifiers_test),axis=0)
    dlls_train = np.concatenate((gw_dlls_train, mw_dlls_train),axis=0)
    dlls_test = np.concatenate((gw_dlls_test, mw_dlls_test),axis=0)
    symbols_train = np.concatenate((gw_symbols_train, mw_symbols_train),axis=0)
    symbols_test = np.concatenate((gw_symbols_test, mw_symbols_test),axis=0)
    labels_train = np.concatenate((gw_labels_train, mw_labels_train),axis=0)
    labels_test = np.concatenate((gw_labels_test, mw_labels_test),axis=0)

    # print number of train and test examples
    print "Train:",features_train.shape[0]
    print "Test:",features_test.shape[0]

    identifiers_model = gensim.models.Word2Vec.load('w2v/identifiers.100.5.model')
    identifiers_w2v = dict(zip(identifiers_model.index2word, identifiers_model.syn0))
    identifiers_vectorizer = MeanEmbeddingVectorizer(identifiers_w2v)

    identifiers_vectorizer.fit(identifiers_train)
    identifiers_train = identifiers_vectorizer.transform(identifiers_train)
    identifiers_test = identifiers_vectorizer.transform(identifiers_test)

    dlls_model = gensim.models.Word2Vec.load('w2v/dlls.100.5.model')
    dlls_w2v = dict(zip(dlls_model.index2word, dlls_model.syn0))
    dlls_vectorizer = MeanEmbeddingVectorizer(dlls_w2v)

    dlls_vectorizer.fit(dlls_train)
    dlls_train = dlls_vectorizer.transform(dlls_train)
    dlls_test = dlls_vectorizer.transform(dlls_test)

    symbols_model = gensim.models.Word2Vec.load('w2v/symbols.100.5.model')
    symbols_w2v = dict(zip(symbols_model.index2word, symbols_model.syn0))
    #symbols_vectorizer = MeanEmbeddingVectorizer(symbols_w2v)
    symbols_vectorizer = TfidfEmbeddingVectorizer(symbols_w2v)

    symbols_vectorizer.fit(symbols_train)
    symbols_train = symbols_vectorizer.transform(symbols_train)
    symbols_test = symbols_vectorizer.transform(symbols_test)


    # concatenate numerical and textual features
    X_train = concatenate_features(features_train,[identifiers_train, dlls_train, symbols_train])
    X_test = concatenate_features(features_test,[identifiers_test, dlls_test, symbols_test])

    # normalize features
    if classifier == "svcrbf":
        normalization = MinMaxScaler(feature_range=(-1, 1))
    else:
        normalization = MinMaxScaler()
    normalization.fit(X_train)
    X_train = normalization.transform(X_train)
    X_test = normalization.transform(X_test)


    # use in a classifier
    if classifier == "svc":
        clf = SVC(kernel="linear", probability=THRESHOLD)
    elif classifier == "svcrbf":
        clf = SVC(C=1000.0, cache_size=200, class_weight=None, coef0=0.0,
          decision_function_shape=None, degree=3, gamma=1.0, kernel='rbf',
          max_iter=-1, probability=THRESHOLD, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
    elif classifier == "linearsvc":
        clf = LinearSVC()
    elif classifier == "knn":
        clf = KNeighborsClassifier(5)
    elif classifier == 'decisiontree':
        clf = DecisionTreeClassifier()
    elif classifier == 'randomforest':
        clf = RandomForestClassifier()
    elif classifier == 'mlp':
        labels_train = hot_encondig(labels_train)
        # Building deep neural network
        net = tflearn.input_data(shape=[None, X_train.shape[1]])
        net = tflearn.fully_connected(net, X_train.shape[1]/2, activation='relu')
        net = tflearn.fully_connected(net, X_train.shape[1]/3, activation='relu')
        net = tflearn.fully_connected(net, 2, activation='softmax')
        net = tflearn.regression(net)
        # Training
        clf = tflearn.DNN(net, tensorboard_verbose=0)

    clf.fit(X_train, labels_train)

    fprs = []
    fnrs = []
    thresholds = []
    if THRESHOLD:
        if classifier == 'mlp':
            pred = clf.predict(X_test)
        else:
            pred = clf.predict_proba(X_test)
        for t in np.arange(0,1,0.0005):
            pred_t = prob_to_class_threshold(pred, t)
            print "Threshold:",t
            # print "Accuracy:", acc
            print "Accuracy:", accuracy_score(labels_test, pred_t)
            print "F1Score:", f1_score(labels_test, pred_t)
            print "Recall:", recall_score(labels_test, pred_t)
            print "Precision:", precision_score(labels_test, pred_t)
            cm = confusion_matrix(labels_test, pred_t)
            print cm
            tn = cm[0][0]
            fn = cm[1][0]
            tp = cm[1][1]
            fp = cm[0][1]
            f1 = f1_score(labels_test, pred_t)
            tpr = tp/float(tp+fn)
            fnr = 1 - tpr
            fpr = fp/float(fn+tn)
            tnr = tn/float(fp+tn)
            print "FPR:",fpr
            print "FNR:",fnr
            thresholds.append(t)
            fprs.append(fpr)
            fnrs.append(fnr)
        x = thresholds
        y_fprs = fprs
        y_fnrs = fnrs
        matplotlib.use('Agg')
        plt.figure(1)
        plt.ylabel(u'FPR & FNR')
        plt.xlabel(u'Threshold')
        plt.title(classifier.upper() + ": FPR & FNR x Threshold")
        plt.axvline(0, color="#000000", linewidth=2)
        plt.axhline(0, color="#000000", linewidth=2)
        x_ticks = np.arange(0,1.05,0.1)
        plt.xticks(x_ticks)
        plt.yticks(x_ticks)
        plt.ylim(-0.05,1.05)
        plt.xlim(-0.05,1.05)
        plt.plot(x, y_fprs, 'b-', label="FPR", linewidth=4)
        plt.plot(x, y_fnrs, 'r-', label="FNR", linewidth=4)
        plt.legend(loc=1)
        plt.grid(True)
        plt.savefig('threshold.pdf')
        # plt.show()

    else:
        pred = clf.predict(X_test)
        if classifier == 'mlp':
            pred = prob_to_class(pred)
        accs.append(accuracy_score(labels_test, pred))
        f1s.append(f1_score(labels_test, pred))
        recs.append(recall_score(labels_test, pred))
        precs.append(precision_score(labels_test, pred))
        print "Accuracy:", accuracy_score(labels_test, pred)
        print "F1Score:", f1_score(labels_test, pred)
        print "Recall:", recall_score(labels_test, pred)
        print "Precision:", precision_score(labels_test, pred)
        print confusion_matrix(labels_test, pred)

accs = np.array(accs)
f1s = np.array(f1s)
recs = np.array(recs)
precs = np.array(precs)
print "Acurácia Média:", accs.sum()/float(N_EXECUTIONS)
print "F1Score Médio:", f1s.sum()/float(N_EXECUTIONS)
print "Recall Médio:", recs.sum()/float(N_EXECUTIONS)
print "Precision Médio:", precs.sum()/float(N_EXECUTIONS)
