# -*- coding: UTF-8 -*-.
import argparse
import os
import csv
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.grid_search import GridSearchCV
import tflearn
import re
import math
import random
import matplotlib.pyplot as plt


N_GOODWARES = 100
N_MALWARES = 1000
N_FEATURES = 100
N_EXECUTIONS = 1
GW_TEST_SIZE = 0.1
MW_TEST_SIZE = 0.1
THRESHOLD = True
FEATURES = [ "BaseOfCode", "BaseOfData", "Characteristics",
             "DllCharacteristics", "Entropy", "FileAlignment", "FileType", "FormatedTimeDateSteamp", "Fuzzy", "Identify", "ImageBase", "ImportedDlls", "ImportedSymbols", "MD5", "Machine", "Magic", "Name", "NumberOfRvaAndSizes", "NumberOfSections", "NumberOfSymbols", "PE_TYPE", "PointerToSymbolTable", "SHA1", "Size", "SizeOfCode", "SizeOfHeaders", "SizeOfImage", "SizeOfInitializedData", "SizeOfOptionalHeader", "SizeOfUninitializedData", "TimeDateStamp"]

USED_FEATURES = [ "BaseOfCode", "BaseOfData", "Characteristics",
                  "DllCharacteristics", "FileAlignment", "ImageBase", "Machine", "Magic", "NumberOfRvaAndSizes", "NumberOfSections", "NumberOfSymbols", "PE_TYPE", "PointerToSymbolTable", "Size", "SizeOfCode", "SizeOfHeaders", "SizeOfImage", "SizeOfInitializedData", "SizeOfOptionalHeader", "SizeOfUninitializedData", "TimeDateStamp"]

def params():
    parser = argparse.ArgumentParser()
    parser.add_argument('classifier', help='Classifier')
    parser.add_argument('goodwares_csv', help='Goodwares CSV location')
    parser.add_argument('malwares_csv', help='Malwares CSV Location')
    parser.add_argument('year', help='Malwares max year')
    params = parser.parse_args()
    return params.classifier, params.goodwares_csv, params.malwares_csv, params.year

def clear_text(text):
    text = text.replace("'","").replace("[","").replace("]","")
    text = re.sub("[^A-Za-z0-9]+","", text)
    return text.lower()

def read_file(input_file):
    with open(input_file, 'rb') as file:
        reader = csv.DictReader(file)
        features = []
        identifiers = []
        imported_dlls = []
        imported_symbols = []
        for row in reader:
            example = []
            for f in USED_FEATURES:
                example.append(int(row[f]))
            example.append(float(row["Entropy"]))
            # get compiler and packer info
            id_regex = r"\[\'[^']*\'\]"
            ident = re.findall(id_regex,row["Identify"])
            identify = ""
            for i in ident:
                identify += clear_text(i) + " "
            # get imported dlls
            dll_regex = r"\'[^']*\'"
            i_dlls = re.findall(dll_regex,row["ImportedDlls"])
            dlls = ""
            for d in i_dlls:
                dlls += clear_text(d) + " "
            # get imported symbols
            i_symbols = re.findall(dll_regex,row["ImportedSymbols"])
            symbols = ""
            for d in i_symbols:
                symbols += clear_text(d) + " "
            # apped texts to their arrays
            identifiers.append(identify)
            imported_dlls.append(dlls)
            imported_symbols.append(symbols)
            # append current example features
            features.append(example)
    features = np.array(features)
    identifiers = np.array(identifiers)
    imported_dlls = np.array(imported_dlls)
    imported_symbols = np.array(imported_symbols)
    return features, identifiers, imported_dlls, imported_symbols

def read_malwares(input_file, max_year=2014):
    with open(input_file, 'rb') as file:
        reader = csv.DictReader(file)
        # train <= max_year
        train_features = []
        train_identifiers = []
        train_imported_dlls = []
        train_imported_symbols = []
        # test > max_year
        test_features = []
        test_identifiers = []
        test_imported_dlls = []
        test_imported_symbols = []
        for row in reader:
            example = []
            for f in USED_FEATURES:
                example.append(int(row[f]))
            example.append(float(row["Entropy"]))
            # get compiler and packer info
            id_regex = r"\[\'[^']*\'\]"
            ident = re.findall(id_regex,row["Identify"])
            identify = ""
            for i in ident:
                identify += clear_text(i) + " "
            # get imported dlls
            dll_regex = r"\'[^']*\'"
            i_dlls = re.findall(dll_regex,row["ImportedDlls"])
            dlls = ""
            for d in i_dlls:
                dlls += clear_text(d) + " "
            # get imported symbols
            i_symbols = re.findall(dll_regex,row["ImportedSymbols"])
            symbols = ""
            for d in i_symbols:
                symbols += clear_text(d) + " "
            year = int(row["FormatedTimeDateStamp"].split('-')[0])
            if year <= max_year:
                # append current example features
                train_features.append(example)
                # apped texts to their arrays
                train_identifiers.append(identify)
                train_imported_dlls.append(dlls)
                train_imported_symbols.append(symbols)
            else:
                # append current example features
                test_features.append(example)
                # apped texts to their arrays
                test_identifiers.append(identify)
                test_imported_dlls.append(dlls)
                test_imported_symbols.append(symbols)
    train_features = np.array(train_features)
    train_identifiers = np.array(train_identifiers)
    train_imported_dlls = np.array(train_imported_dlls)
    train_imported_symbols = np.array(train_imported_symbols)
    test_features = np.array(test_features)
    test_identifiers = np.array(test_identifiers)
    test_imported_dlls = np.array(test_imported_dlls)
    test_imported_symbols = np.array(test_imported_symbols)
    return train_features, train_identifiers, train_imported_dlls, train_imported_symbols, test_features, test_identifiers, test_imported_dlls, test_imported_symbols

def cross_validation(features, identifiers, dlls, symbols, labels, test_size):
    # total data
    n_data = len(features)
    # number of test data
    n_test = math.ceil(n_data*test_size)
    # get random n_test indexes
    indexes = random.sample(range(n_data), int(n_test))
    features_test = []
    features_train = []
    identifiers_test = []
    identifiers_train = []
    dlls_test = []
    dlls_train = []
    symbols_test = []
    symbols_train = []
    labels_test = []
    labels_train = []
    for i in range(len(features)):
        if i not in indexes:
            # add data i to train
            features_train.append(features[i])
            identifiers_train.append(identifiers[i])
            dlls_train.append(dlls[i])
            symbols_train.append(symbols[i])
            labels_train.append(labels[i])
        else:
            # add data i to test
            features_test.append(features[i])
            identifiers_test.append(identifiers[i])
            dlls_test.append(dlls[i])
            symbols_test.append(symbols[i])
            labels_test.append(labels[i])
    return features_train, features_test, identifiers_train, identifiers_test, dlls_train, dlls_test, symbols_train, symbols_test, labels_train, labels_test

def concatenate_features(features, array):
    X = []
    for i in range(features.shape[0]):
        x = features[i]
        for j in array:
            x = np.concatenate((x,j[i].toarray()[0]))
        X.append(x)
    return np.array(X)


def hot_encondig(y):
    new_y = []
    for i in y:
        temp = np.zeros(2)
        temp[int(i)] = 1
        new_y.append(temp)
    return np.array(new_y)

def prob_to_class(pred):
    new_pred = []
    for i in pred:
        temp = np.array(i).argsort()[::-1][0]
        new_pred.append(temp)
    return new_pred

def prob_to_class_threshold(pred, threshold):
    new_pred = []
    for i in pred:
        dist = i[np.array(i).argsort()[::-1][0]]
        if dist > threshold:
            temp = np.array(i).argsort()[::-1][0]
        else:
            temp = 0
        new_pred.append(temp)
    return new_pred


# get params
classifier, gw_csv, mw_csv, year = params()

# read goodwares and malwares
gw_features, gw_identifiers, gw_dlls, gw_symbols = read_file(gw_csv)
mw_features_train, mw_identifiers_train, mw_dlls_train, mw_symbols_train, mw_features_test, mw_identifiers_test, mw_dlls_test, mw_symbols_test = read_malwares(mw_csv, max_year=int(year))

print "Goodwares:",len(gw_features)
print "Malwares:", len(mw_features_train)+len(mw_features_test)

# create label arrays
gw_label = np.zeros(len(gw_features))
mw_labels_train = np.ones(len(mw_features_train))
mw_labels_test = np.ones(len(mw_features_test))

for exe in range(N_EXECUTIONS):

    # cross validation

    # goodwares cross validation
    gw_features_train, gw_features_test, gw_identifiers_train, gw_identifiers_test, gw_dlls_train, gw_dlls_test, gw_symbols_train, gw_symbols_test, gw_labels_train, gw_labels_test = cross_validation(gw_features, gw_identifiers, gw_dlls, gw_symbols, gw_label,test_size=GW_TEST_SIZE)

    # create train and test sets - concatenate goodwares and malwares
    features_train = np.concatenate((gw_features_train, mw_features_train),axis=0)
    features_test = np.concatenate((gw_features_test, mw_features_test),axis=0)
    identifiers_train = np.concatenate((gw_identifiers_train, mw_identifiers_train),axis=0)
    identifiers_test = np.concatenate((gw_identifiers_test, mw_identifiers_test),axis=0)
    dlls_train = np.concatenate((gw_dlls_train, mw_dlls_train),axis=0)
    dlls_test = np.concatenate((gw_dlls_test, mw_dlls_test),axis=0)
    symbols_train = np.concatenate((gw_symbols_train, mw_symbols_train),axis=0)
    symbols_test = np.concatenate((gw_symbols_test, mw_symbols_test),axis=0)
    labels_train = np.concatenate((gw_labels_train, mw_labels_train),axis=0)
    labels_test = np.concatenate((gw_labels_test, mw_labels_test),axis=0)

    # print number of train and test examples
    print "Train:",features_train.shape[0]
    print "Test:",features_test.shape[0]

    # train tfidf for each textual feature
    identifiers_tfidf = TfidfVectorizer(max_features=N_FEATURES)
    identifiers_tfidf.fit(identifiers_train)
    identifiers_train = identifiers_tfidf.transform(identifiers_train)
    identifiers_test = identifiers_tfidf.transform(identifiers_test)
    dlls_tfidf = TfidfVectorizer(max_features=N_FEATURES)
    dlls_tfidf.fit(dlls_train)
    dlls_train = dlls_tfidf.transform(dlls_train)
    dlls_test = dlls_tfidf.transform(dlls_test)
    symbols_tfidf = TfidfVectorizer(max_features=N_FEATURES)
    symbols_tfidf.fit(symbols_train)
    symbols_train = symbols_tfidf.transform(symbols_train)
    symbols_test = symbols_tfidf.transform(symbols_test)

    # concatenate numerical and textual features
    X_train = concatenate_features(features_train,[identifiers_train, dlls_train, symbols_train])
    X_test = concatenate_features(features_test,[identifiers_test, dlls_test, symbols_test])

    # normalize features
    if classifier == "svcrbf":
        normalization = MinMaxScaler(feature_range=(-1, 1))
    else:
        normalization = MinMaxScaler()
    normalization.fit(X_train)
    X_train = normalization.transform(X_train)
    X_test = normalization.transform(X_test)


    # use in a classifier
    if classifier == "svc":
        clf = SVC(kernel="linear", probability=THRESHOLD)
    elif classifier == "svcrbf":
        clf = SVC(C=1000.0, cache_size=200, class_weight=None, coef0=0.0,
          decision_function_shape=None, degree=3, gamma=1.0, kernel='rbf',
          max_iter=-1, probability=THRESHOLD, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
    elif classifier == "linearsvc":
        clf = LinearSVC()
    elif classifier == "knn":
        clf = KNeighborsClassifier(5)
    elif classifier == 'decisiontree':
        clf = DecisionTreeClassifier()
    elif classifier == 'randomforest':
        clf = RandomForestClassifier()
    elif classifier == 'mlp':
        labels_train = hot_encondig(labels_train)
        # Building deep neural network
        net = tflearn.input_data(shape=[None, X_train.shape[1]])
        net = tflearn.fully_connected(net, X_train.shape[1]/2, activation='relu')
        net = tflearn.fully_connected(net, X_train.shape[1]/3, activation='relu')
        net = tflearn.fully_connected(net, 2, activation='softmax')
        net = tflearn.regression(net)
        # Training
        clf = tflearn.DNN(net, tensorboard_verbose=0)


    accs = []
    f1s = []
    precs = []
    recs = []

    clf.fit(X_train, labels_train)

    fprs = []
    fnrs = []
    thresholds = []
    if THRESHOLD:
        if classifier == 'mlp':
            pred = clf.predict(X_test)
        else:
            pred = clf.predict_proba(X_test)
        for t in np.arange(0,1,0.0005):
            pred_t = prob_to_class_threshold(pred, t)
            print "Threshold:",t
            # print "Accuracy:", acc
            print "Accuracy:", accuracy_score(labels_test, pred_t)
            print "F1Score:", f1_score(labels_test, pred_t)
            cm = confusion_matrix(labels_test, pred_t)
            print cm
            fpr = cm[0][1]/float(cm[0].sum())
            fnr = cm[1][0]/float(cm[1].sum())
            print "FPR:",fpr
            print "FNR:",fnr
            thresholds.append(t)
            fprs.append(fpr)
            fnrs.append(fnr)
        x = thresholds
        y_fprs = fprs
        y_fnrs = fnrs
        plt.figure(1)
        plt.ylabel(u'FPR & FNR')
        plt.xlabel(u'Threshold')
        plt.title(classifier.upper() + ": FPR & FNR x Threshold")
        plt.axvline(0, color="#000000", linewidth=2)
        plt.axhline(0, color="#000000", linewidth=2)
        x_ticks = np.arange(0,1.05,0.1)
        plt.xticks(x_ticks)
        plt.yticks(x_ticks)
        plt.ylim(-0.05,1.05)
        plt.xlim(-0.05,1.05)
        plt.plot(x, y_fprs, 'b-', label="FPR", linewidth=4)
        plt.plot(x, y_fnrs, 'r-', label="FNR", linewidth=4)
        plt.legend(loc=1)
        plt.grid(True)
        plt.show()

    else:
        pred = clf.predict(X_test)
        if classifier == 'mlp':
            pred = prob_to_class(pred)
        accs.append(accuracy_score(labels_test, pred))
        f1s.append(f1_score(labels_test, pred))
        recs.append(recall_score(labels_test, pred))
        precs.append(precision_score(labels_test, pred))
        print "Accuracy:", accuracy_score(labels_test, pred)
        print "F1Score:", f1_score(labels_test, pred)
        print "Recall:", recall_score(labels_test, pred)
        print "Precision:", precision_score(labels_test, pred)
        print confusion_matrix(labels_test, pred)

accs = np.array(accs)
f1s = np.array(f1s)
recs = np.array(recs)
precs = np.array(precs)
print "Acurácia Média:", accs.sum()/float(N_EXECUTIONS)
print "F1Score Médio:", f1s.sum()/float(N_EXECUTIONS)
print "Recall Médio:", recs.sum()/float(N_EXECUTIONS)
print "Precision Médio:", precs.sum()/float(N_EXECUTIONS)
