# -*- coding: UTF-8 -*-.
import gensim
from tensorflow.contrib.tensorboard.plugins import projector
import tensorflow as tf
import os
import csv

LOG_DIR="log"

model = gensim.models.Word2Vec.load('symbols.100.5.model')

matrix = model.syn0
symbols = model.index2word

with open(LOG_DIR+"/metadata.tsv", "w") as fh:
    fh.write("Name\tLabel\n")
    for i in symbols:
        fh.write("".join(i+"\t"+"0") + "\n")

print model.syn0[0]
print model.index2word[0]

embedding_var = tf.Variable(matrix)

saver = tf.train.Saver()
session = tf.Session()
init = tf.global_variables_initializer()
session.run(init)
saver.save(session, os.path.join(LOG_DIR, "model.ckpt"), 0)

# Use the same LOG_DIR where you stored your checkpoint.
summary_writer = tf.summary.FileWriter(LOG_DIR)

# Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto
config = projector.ProjectorConfig()

# You can add multiple embeddings. Here we add only one.
embedding = config.embeddings.add()
embedding.tensor_name = "Symbols W2V"
# Link this tensor to its metadata file (e.g. labels).
embedding.metadata_path = os.path.join(LOG_DIR,'metadata.tsv')

# Saves a configuration file that TensorBoard will read during startup.
projector.visualize_embeddings(summary_writer, config)
