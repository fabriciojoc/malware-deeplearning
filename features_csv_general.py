# -*- coding: UTF-8 -*-.
import argparse
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import sys
import csv
import gensim
sys.path.append("./lib")
from common import read_file, cross_validation, hot_enconding, concatenate_features, concatenate_features_w2v, prob_to_class, prob_to_class_threshold, headers, random_samples

N_FEATURES = 100
SAMPLING_SIZE=2500
TFIDF = True

class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        # if a text is empty we should return a vector of zeros
        # with the same dimensionality as all the other vectors
        self.dim = len(word2vec.itervalues().next())

    def fit(self, X):
        return self

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])

class TfidfEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.word2weight = None
        self.dim = len(word2vec.itervalues().next())

    def fit(self, X):
        tfidf = TfidfVectorizer(analyzer=lambda x: x)
        tfidf.fit(X)
        # if a word was never seen - it must be at least as infrequent
        # as any of the known words - so the default idf is the max of
        # known idf's
        max_idf = max(tfidf.idf_)
        self.word2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

        return self

    def transform(self, X):
        return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
            ])

def params():
    parser = argparse.ArgumentParser()
    parser.add_argument('goodwares_csv', help='Goodwares CSV location')
    parser.add_argument('malwares_csv', help='Malwares CSV Location')
    params = parser.parse_args()
    return params.goodwares_csv, params.malwares_csv

# get params
gw_csv, mw_csv = params()

# create csv file header
h = headers(gw_csv)
for i in range(1,N_FEATURES+1):
    h.append("identifiers"+str(i))
for i in range(1,N_FEATURES+1):
    h.append("dlls"+str(i))
for i in range(1,N_FEATURES+1):
    h.append("symbols"+str(i))
h.append("label")

# read goodwares and malwares
gw_features, gw_identifiers, gw_dlls, gw_symbols = read_file(gw_csv)
mw_features, mw_identifiers, mw_dlls, mw_symbols = read_file(mw_csv)

print "Goodwares:",len(gw_features)
print "Malwares:", len(mw_features)

# create label arrays
gw_labels = np.zeros(len(gw_features))
mw_labels = np.ones(len(mw_features))

gw_features, gw_identifiers, gw_dlls, gw_symbols, gw_labels = random_samples(gw_features, gw_identifiers, gw_dlls, gw_symbols, gw_labels, SAMPLING_SIZE)

mw_features, mw_identifiers, mw_dlls, mw_symbols, mw_labels = random_samples(mw_features, mw_identifiers, mw_dlls, mw_symbols, mw_labels, SAMPLING_SIZE)

features = np.concatenate((gw_features, mw_features),axis=0)
identifiers = np.concatenate((gw_identifiers, mw_identifiers),axis=0)
dlls = np.concatenate((gw_dlls, mw_dlls),axis=0)
symbols = np.concatenate((gw_symbols, mw_symbols),axis=0)
labels = np.concatenate((gw_labels, mw_labels),axis=0)

# TFIDF

# train tfidf for each textual feature
identifiers_tfidf = TfidfVectorizer(max_features=N_FEATURES)
identifiers_tfidf.fit(identifiers)
identifiers_tf = identifiers_tfidf.transform(identifiers)

dlls_tfidf = TfidfVectorizer(max_features=N_FEATURES)
dlls_tfidf.fit(dlls)
dlls_tf = dlls_tfidf.transform(dlls)

symbols_tfidf = TfidfVectorizer(max_features=N_FEATURES)
symbols_tfidf.fit(symbols)
symbols_tf = symbols_tfidf.transform(symbols)

# concatenate numerical and textual features
X_tf = concatenate_features(features,[identifiers_tf, dlls_tf, symbols_tf])

# W2V

identifiers_model = gensim.models.Word2Vec.load('w2v/identifiers.100.5.model')
identifiers_w2v = dict(zip(identifiers_model.index2word, identifiers_model.syn0))
identifiers_vectorizer = MeanEmbeddingVectorizer(identifiers_w2v)
identifiers_vectorizer.fit(identifiers)
identifiers_w2v = identifiers_vectorizer.transform(identifiers)

dlls_model = gensim.models.Word2Vec.load('w2v/dlls.100.5.model')
dlls_w2v = dict(zip(dlls_model.index2word, dlls_model.syn0))
dlls_vectorizer = MeanEmbeddingVectorizer(dlls_w2v)
dlls_vectorizer.fit(dlls)
dlls_w2v = dlls_vectorizer.transform(dlls)

symbols_model = gensim.models.Word2Vec.load('w2v/symbols.100.5.model')
symbols_w2v = dict(zip(symbols_model.index2word, symbols_model.syn0))
symbols_vectorizer = MeanEmbeddingVectorizer(symbols_w2v)
symbols_vectorizer.fit(symbols)
symbols_w2v = symbols_vectorizer.transform(symbols)

# concatenate numerical and textual features
X_w2v = concatenate_features_w2v(features,[identifiers_w2v, dlls_w2v, symbols_w2v])

csv_tf = open('features_tfidf_5k.csv', "a")
csv_w2v = open('features_w2v_5k.csv', "a")
c_tf = csv.writer(csv_tf)
c_w2v = csv.writer(csv_w2v)
c_tf.writerow(h)
c_w2v.writerow(h)

for i in range(X_tf.shape[0]):
    c_tf.writerow(np.concatenate((X_tf[i], [labels[i]]),axis=0))

for i in range(X_w2v.shape[0]):
    c_w2v.writerow(np.concatenate((X_w2v[i], [labels[i]]),axis=0))
